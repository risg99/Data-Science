{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the dataset are described with respect to see how much it affects the sales:\n",
    "\n",
    "1. Item_ID : Maps to a unique entry\n",
    "\n",
    "2. Item_Weight : Not necessary to predict the sales \n",
    "\n",
    "3. Item_Fat_Content : Low Fat products are more likely to be consumed than High Fat products, thus affecting the sales\n",
    "\n",
    "4. Item_Visibility : Products placed at a proper place, such that it is easier for users to view and locate them increases their chances of being sold, thus affecting the sales\n",
    "\n",
    "5. Item_Type : Dairy product like butter or Cleansing product like soaps, daily based products(Household items wrt the given data) are bought more than specific use products, thus affecting the sales\n",
    "\n",
    "6. Item_MRP : Has no effect on the sales\n",
    "\n",
    "7. Outlet_Identifier : ID for the Outlet for which sales is being predicted\n",
    "\n",
    "8. Outlet_Establishment_Year : Not affecting the sales\n",
    "\n",
    "9. Outlet_Size : A bigger outlet implies it can cater to the needs of many people simultaneously, hence bigger outlet means more sales, thus affecting the sales\n",
    "\n",
    "10. Outlet_Location_Type : Tier 1 implying the most densely populated area, while Tier 3 being the less densely populated area. Most densely populated area means more people will buy the product in comparison to the less densely populated, thus affecting the sales\n",
    "\n",
    "11. Outlet_Type : Similar to Outlet_Size, affects the sales\n",
    "\n",
    "12. Outlet_Sales : Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files:\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to work on a single data file as train and test instead of two different. A plus point is it avoids confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 13) (5681, 12) (14204, 13)\n"
     ]
    }
   ],
   "source": [
    "train['source']='train'\n",
    "test['source']='test'\n",
    "data = pd.concat([train, test],ignore_index=True)\n",
    "print(train.shape, test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Fat_Content                0\n",
       "Item_Identifier                 0\n",
       "Item_MRP                        0\n",
       "Item_Outlet_Sales            5681\n",
       "Item_Type                       0\n",
       "Item_Visibility                 0\n",
       "Item_Weight                  2439\n",
       "Outlet_Establishment_Year       0\n",
       "Outlet_Identifier               0\n",
       "Outlet_Location_Type            0\n",
       "Outlet_Size                  4016\n",
       "Outlet_Type                     0\n",
       "source                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the null values in the dataset\n",
    "data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the first five rows of the dataset\n",
      "  Item_Fat_Content Item_Identifier  Item_MRP  Item_Outlet_Sales  \\\n",
      "0          Low Fat           FDA15  249.8092          3735.1380   \n",
      "1          Regular           DRC01   48.2692           443.4228   \n",
      "2          Low Fat           FDN15  141.6180          2097.2700   \n",
      "3          Regular           FDX07  182.0950           732.3800   \n",
      "4          Low Fat           NCD19   53.8614           994.7052   \n",
      "\n",
      "               Item_Type  Item_Visibility  Item_Weight  \\\n",
      "0                  Dairy         0.016047         9.30   \n",
      "1            Soft Drinks         0.019278         5.92   \n",
      "2                   Meat         0.016760        17.50   \n",
      "3  Fruits and Vegetables         0.000000        19.20   \n",
      "4              Household         0.000000         8.93   \n",
      "\n",
      "   Outlet_Establishment_Year Outlet_Identifier Outlet_Location_Type  \\\n",
      "0                       1999            OUT049               Tier 1   \n",
      "1                       2009            OUT018               Tier 3   \n",
      "2                       1999            OUT049               Tier 1   \n",
      "3                       1998            OUT010               Tier 3   \n",
      "4                       1987            OUT013               Tier 3   \n",
      "\n",
      "  Outlet_Size        Outlet_Type source  \n",
      "0      Medium  Supermarket Type1  train  \n",
      "1      Medium  Supermarket Type2  train  \n",
      "2      Medium  Supermarket Type1  train  \n",
      "3         NaN      Grocery Store  train  \n",
      "4        High  Supermarket Type1  train  \n",
      "------------------------------------------------\n",
      "Printing the shape of the dataset\n",
      "(14204, 13)\n",
      "------------------------------------------------\n",
      "Printing the statistics of the dataset\n",
      "           Item_MRP  Item_Outlet_Sales  Item_Visibility   Item_Weight  \\\n",
      "count  14204.000000        8523.000000     14204.000000  11765.000000   \n",
      "mean     141.004977        2181.288914         0.065953     12.792854   \n",
      "std       62.086938        1706.499616         0.051459      4.652502   \n",
      "min       31.290000          33.290000         0.000000      4.555000   \n",
      "25%       94.012000         834.247400         0.027036      8.710000   \n",
      "50%      142.247000        1794.331000         0.054021     12.600000   \n",
      "75%      185.855600        3101.296400         0.094037     16.750000   \n",
      "max      266.888400       13086.964800         0.328391     21.350000   \n",
      "\n",
      "       Outlet_Establishment_Year  \n",
      "count               14204.000000  \n",
      "mean                 1997.830681  \n",
      "std                     8.371664  \n",
      "min                  1985.000000  \n",
      "25%                  1987.000000  \n",
      "50%                  1999.000000  \n",
      "75%                  2004.000000  \n",
      "max                  2009.000000  \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the first five rows of the dataset\")\n",
    "print(data.head())\n",
    "print('------------------------------------------------')\n",
    "print(\"Printing the shape of the dataset\")\n",
    "print(data.shape)\n",
    "print('------------------------------------------------')\n",
    "print(\"Printing the statistics of the dataset\")\n",
    "print(data.describe())\n",
    "print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data.describe() we infer that: \n",
    "\n",
    "1. Item_Visibility's minimum value equals to 0. But, it is ethically incorrect as if a product is kept in a store to be sold, it is visible, that is it cannot be zero.\n",
    "    \n",
    "2. Outlet_Establishment_Year ranges from 1985 to 2009. This we cannot use as it is to train the data, instead the outlet is 1 year old or more is more beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of finding unique values in different columns of the dataset, we apply lambda function and find the length of unqiue values that iterates through all the columns of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Fat_Content                 5\n",
       "Item_Identifier               1559\n",
       "Item_MRP                      8052\n",
       "Item_Outlet_Sales             3494\n",
       "Item_Type                       16\n",
       "Item_Visibility              13006\n",
       "Item_Weight                    416\n",
       "Outlet_Establishment_Year        9\n",
       "Outlet_Identifier               10\n",
       "Outlet_Location_Type             3\n",
       "Outlet_Size                      4\n",
       "Outlet_Type                      4\n",
       "source                           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting this, we come to know that unique values for instance Outlet_Location_Type is 3 : Tier_1, Tier_2 and Tier_3 that sums upto 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Fat_Content              object\n",
       "Item_Identifier               object\n",
       "Item_MRP                     float64\n",
       "Item_Outlet_Sales            float64\n",
       "Item_Type                     object\n",
       "Item_Visibility              float64\n",
       "Item_Weight                  float64\n",
       "Outlet_Establishment_Year      int64\n",
       "Outlet_Identifier             object\n",
       "Outlet_Location_Type          object\n",
       "Outlet_Size                   object\n",
       "Outlet_Type                   object\n",
       "source                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Item_Fat_Content', 'Item_Identifier', 'Item_Type', 'Outlet_Identifier', 'Outlet_Location_Type', 'Outlet_Size', 'Outlet_Type', 'source']\n"
     ]
    }
   ],
   "source": [
    "#Filter categorical variables i.e. columns with datatypes as object\n",
    "categorical_columns = [x for x in data.dtypes.index if data.dtypes[x]=='object']\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of Categories for variable:  Item_Fat_Content\n",
      "Low Fat    8485\n",
      "Regular    4824\n",
      "LF          522\n",
      "reg         195\n",
      "low fat     178\n",
      "Name: Item_Fat_Content, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable:  Item_Type\n",
      "Fruits and Vegetables    2013\n",
      "Snack Foods              1989\n",
      "Household                1548\n",
      "Frozen Foods             1426\n",
      "Dairy                    1136\n",
      "Baking Goods             1086\n",
      "Canned                   1084\n",
      "Health and Hygiene        858\n",
      "Meat                      736\n",
      "Soft Drinks               726\n",
      "Breads                    416\n",
      "Hard Drinks               362\n",
      "Others                    280\n",
      "Starchy Foods             269\n",
      "Breakfast                 186\n",
      "Seafood                    89\n",
      "Name: Item_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable:  Outlet_Location_Type\n",
      "Tier 3    5583\n",
      "Tier 2    4641\n",
      "Tier 1    3980\n",
      "Name: Outlet_Location_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable:  Outlet_Size\n",
      "Medium    4655\n",
      "Small     3980\n",
      "High      1553\n",
      "Name: Outlet_Size, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable:  Outlet_Type\n",
      "Supermarket Type1    9294\n",
      "Grocery Store        1805\n",
      "Supermarket Type3    1559\n",
      "Supermarket Type2    1546\n",
      "Name: Outlet_Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Exclude ID cols and source:\n",
    "categorical_columns = [x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier','source']]\n",
    "#Print frequency of categories\n",
    "for col in categorical_columns:\n",
    "    print('\\nFrequency of Categories for variable: ',col)\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretations from the above data are:\n",
    "    \n",
    "1. Low Fat, LF and low fat are three different categories for one single category Low Fat.\n",
    "\n",
    "2. Regular and reg are two different categories for one single category Regular.\n",
    "\n",
    "3. For Item_Type, all of them do not have substantial numbers. Combining them, might help.\n",
    "\n",
    "4. For Outlet_Type, Supermarket Type2 and Type3 can be combined. But, is it a good choice to combine or not must be thought of before combining. Also, Supermaket Type1 and the mixed category can be combined into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Item_Weight\n",
      "Item_Identifier             \n",
      "DRA12                 11.600\n",
      "DRA24                 19.350\n",
      "DRA59                  8.270\n",
      "DRB01                  7.390\n",
      "DRB13                  6.115\n",
      "...                      ...\n",
      "NCZ30                  6.590\n",
      "NCZ41                 19.850\n",
      "NCZ42                 10.500\n",
      "NCZ53                  9.600\n",
      "NCZ54                 14.650\n",
      "\n",
      "[1559 rows x 1 columns]\n",
      "0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "14199    False\n",
      "14200    False\n",
      "14201    False\n",
      "14202    False\n",
      "14203    False\n",
      "Name: Item_Weight, Length: 14204, dtype: bool\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'FDP10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FDP10'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-957be222f5a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Impute data and check missing values before and after imputation to confirm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# print('Orignal missing: %d'% sum(miss_bool))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmiss_bool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Item_Weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmiss_bool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Item_Identifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem_avg_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# print('Final missing: %d'% sum(data['Item_Weight'].isnull()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4043\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4045\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4047\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-957be222f5a0>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Impute data and check missing values before and after imputation to confirm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# print('Orignal missing: %d'% sum(miss_bool))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmiss_bool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Item_Weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmiss_bool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Item_Identifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem_avg_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# print('Final missing: %d'% sum(data['Item_Weight'].isnull()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2994\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2995\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2996\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FDP10'"
     ]
    }
   ],
   "source": [
    "#Determine the average weight per item:\n",
    "item_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
    "print(item_avg_weight)\n",
    "# print(data['Item_Identifier']=='FDP10')\n",
    "\n",
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "miss_bool = data['Item_Weight'].isnull() \n",
    "print(miss_bool)\n",
    "\n",
    "#Impute data and check missing values before and after imputation to confirm\n",
    "# print('Orignal missing: %d'% sum(miss_bool))\n",
    "data.loc[miss_bool,'Item_Weight'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x: item_avg_weight[x] if x else 0)\n",
    "# print('Final missing: %d'% sum(data['Item_Weight'].isnull()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode for each Outlet_Type:\n",
      "Outlet_Type Grocery Store Supermarket Type1 Supermarket Type2  \\\n",
      "Outlet_Size         Small             Small            Medium   \n",
      "\n",
      "Outlet_Type Supermarket Type3  \n",
      "Outlet_Size            Medium  \n",
      "\n",
      "Orignal #missing: 4016\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Import mode function:\n",
    "from scipy.stats import mode\n",
    "\n",
    "#Determing the mode for each\n",
    "outlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x).mode[0]))\n",
    "print('Mode for each Outlet_Type:')\n",
    "print(outlet_size_mode)\n",
    "\n",
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "miss_bool = data['Outlet_Size'].isnull() \n",
    "\n",
    "#Impute data and check #missing values before and after imputation to confirm\n",
    "print('\\nOrignal #missing: %d'% sum(miss_bool))\n",
    "data.loc[miss_bool,'Outlet_Size'] = data.loc[miss_bool,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\n",
    "print(sum(data['Outlet_Size'].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
